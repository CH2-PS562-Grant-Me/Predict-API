{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a10bc2ee",
      "metadata": {
        "id": "a10bc2ee"
      },
      "source": [
        "# Recommendation System for Scholarship : Grant Me App\n",
        "*Use DUMMY DATA*\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe83835c",
      "metadata": {},
      "source": [
        "Import *Library*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "d43abc49",
      "metadata": {
        "id": "d43abc49",
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib as plt\n",
        "import pathlib"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ec75681",
      "metadata": {},
      "source": [
        "Read the **Datasets**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "81ec0bd5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read the data from the CSV file or use an existing dataframe\n",
        "# using the dummy data for now\n",
        "data = pd.read_csv('v2merged2.csv')\n",
        "\n",
        "# Separate the columns using semicolons\n",
        "data[['Column1', 'Volunteers', 'Internships', 'National_Honor', 'National_Top3', 'International_Honor',\n",
        "      'International_Top3', 'Certification',\n",
        "      'Professional_Certification', 'Scholarships', 'Scholarship_Type', 'Scholarship_Name', 'GPA']] = data['Column1;Volunteers;Internships;National_Honor;National_Top3;International_Honor;International_Top3;Certification;Professional_Certification;Scholarships;Scholarship_Type;Scholarship_Name;GPA'].str.split(';', expand=True)\n",
        "\n",
        "# Drop the unnecessary columns\n",
        "data = data.drop(columns=['Column1;Volunteers;Internships;National_Honor;National_Top3;International_Honor;International_Top3;Certification;Professional_Certification;Scholarships;Scholarship_Type;Scholarship_Name;GPA'])\n",
        "\n",
        "# Replace empty string values with NaN\n",
        "data.replace('', np.nan, inplace=True)\n",
        "\n",
        "# Convert data types to float if needed\n",
        "data[['Volunteers', 'Internships', 'National_Honor', 'National_Top3', 'International_Honor',\n",
        "    'International_Top3', 'Certification', 'Professional_Certification',\n",
        "    'GPA']] = data[['Volunteers', 'Internships', 'National_Honor', 'National_Top3', 'International_Honor',\n",
        "    'International_Top3', 'Certification', 'Professional_Certification',\n",
        "    'GPA']].astype(float)\n",
        "\n",
        "# # Replace NaN values with mean or other replacement strategy\n",
        "# data.fillna(data.mean(), inplace=True)\n",
        "\n",
        "# Get the columns to be used for clustering\n",
        "X = data[['GPA', 'Certification', 'Professional_Certification', 'National_Honor', 'National_Top3',\n",
        "    'International_Honor', 'International_Top3', 'Internships',\n",
        "    'Volunteers']]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f07c827",
      "metadata": {},
      "source": [
        "Make the **Clustering Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "68d7c6af",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Akbar Aj\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  super()._check_params_vs_input(X, default_n_init=10)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0        1\n",
            "1        1\n",
            "2        1\n",
            "3        1\n",
            "4        1\n",
            "        ..\n",
            "14009    1\n",
            "14010    1\n",
            "14011    4\n",
            "14012    4\n",
            "14013    1\n",
            "Name: Cluster, Length: 14014, dtype: int32\n",
            "Epoch 1/10\n",
            "351/351 [==============================] - 2s 3ms/step - loss: 0.3519 - accuracy: 0.8898 - val_loss: 0.0777 - val_accuracy: 0.9700\n",
            "Epoch 2/10\n",
            "351/351 [==============================] - 1s 2ms/step - loss: 0.0544 - accuracy: 0.9801 - val_loss: 0.0490 - val_accuracy: 0.9797\n",
            "Epoch 3/10\n",
            "351/351 [==============================] - 1s 2ms/step - loss: 0.0423 - accuracy: 0.9843 - val_loss: 0.0419 - val_accuracy: 0.9857\n",
            "Epoch 4/10\n",
            "351/351 [==============================] - 1s 2ms/step - loss: 0.0358 - accuracy: 0.9857 - val_loss: 0.0362 - val_accuracy: 0.9839\n",
            "Epoch 5/10\n",
            "351/351 [==============================] - 1s 2ms/step - loss: 0.0323 - accuracy: 0.9872 - val_loss: 0.0367 - val_accuracy: 0.9857\n",
            "Epoch 6/10\n",
            "351/351 [==============================] - 1s 2ms/step - loss: 0.0289 - accuracy: 0.9881 - val_loss: 0.0308 - val_accuracy: 0.9889\n",
            "Epoch 7/10\n",
            "351/351 [==============================] - 1s 2ms/step - loss: 0.0255 - accuracy: 0.9899 - val_loss: 0.0352 - val_accuracy: 0.9868\n",
            "Epoch 8/10\n",
            "351/351 [==============================] - 1s 3ms/step - loss: 0.0239 - accuracy: 0.9904 - val_loss: 0.0292 - val_accuracy: 0.9886\n",
            "Epoch 9/10\n",
            "351/351 [==============================] - 1s 2ms/step - loss: 0.0237 - accuracy: 0.9909 - val_loss: 0.0349 - val_accuracy: 0.9847\n",
            "Epoch 10/10\n",
            "351/351 [==============================] - 1s 2ms/step - loss: 0.0216 - accuracy: 0.9917 - val_loss: 0.0229 - val_accuracy: 0.9907\n",
            "88/88 [==============================] - 0s 2ms/step - loss: 0.0229 - accuracy: 0.9907\n",
            "Test Loss: 0.02290794439613819\n",
            "Test Accuracy: 0.9907242059707642\n"
          ]
        }
      ],
      "source": [
        "# Standardize the data using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Create a k-means model with the desired number of clusters\n",
        "kmeans = KMeans(n_clusters=5, random_state=42)\n",
        "\n",
        "# Perform clustering on the data\n",
        "kmeans.fit(X_scaled)\n",
        "\n",
        "# Add the clustering result column to the dataframe\n",
        "data['Cluster'] = kmeans.labels_\n",
        "\n",
        "# Display the clustering result\n",
        "print(data['Cluster'])\n",
        "\n",
        "# Import the required libraries for model training and evaluation\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, data['Cluster'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the TensorFlow model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "    tf.keras.layers.Dense(16, activation='relu'),\n",
        "    tf.keras.layers.Dense(5, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=1, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "print('Test Loss:', test_loss)\n",
        "print('Test Accuracy:', test_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9bc8389d",
      "metadata": {},
      "source": [
        "Test the Model with **Random Input** and **Plot the Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "5609a429",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Inputan masuk ke cluster: [1]\n",
            "Swasta\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Akbar Aj\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Inputan baru untuk diprediksi\n",
        "new_input = np.array([[3.8, 5, 3, 2, 1, 1, 1, 14, 13]])  # Ganti dengan inputan yang sesuai\n",
        "\n",
        "# Lakukan standardisasi pada inputan baru\n",
        "new_input_scaled = scaler.transform(new_input)\n",
        "\n",
        "# Lakukan prediksi cluster\n",
        "predicted_cluster = kmeans.predict(new_input_scaled)\n",
        "\n",
        "# Tampilkan hasil prediksi\n",
        "print(\"Inputan masuk ke cluster:\", predicted_cluster)\n",
        "if predicted_cluster == 0:\n",
        "    print(\"Pemerintah\")\n",
        "elif predicted_cluster == 1:\n",
        "    print(\"Swasta\")\n",
        "elif predicted_cluster == 2:\n",
        "    print(\"Organisasi\")\n",
        "elif predicted_cluster == 3:\n",
        "    print(\"Prestasi\")\n",
        "elif predicted_cluster == 4:\n",
        "    print(\"Bantuan\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e429f369",
      "metadata": {},
      "source": [
        "Save the **Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "5d61b532",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Akbar Aj\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "# Save the model\n",
        "model.save('model_V2.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4bfe267",
      "metadata": {},
      "source": [
        "Create **Save Model Folder**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ec52cd3",
      "metadata": {},
      "outputs": [],
      "source": [
        "export_dir = 'saved_model/1'\n",
        "tf.saved_model.save(model, export_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "565197d4",
      "metadata": {},
      "source": [
        "Convert Saved Model to TfLite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f89797e2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert the model.\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(export_dir)\n",
        "tflite_model = converter.convert()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "649dde98",
      "metadata": {},
      "outputs": [],
      "source": [
        "tflite_model_file = pathlib.Path('model.tflite')\n",
        "tflite_model_file.write_bytes(tflite_model)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
